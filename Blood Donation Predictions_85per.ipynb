{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing, svm  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "import math\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '/Users/anyaozmen/Downloads/blood_don/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(address, index_col=0)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from matplotlib.pyplot import figure\n",
    "#plt.figure(num=None, figsize=(40, 30), dpi=60)\n",
    "#plt.scatter(df['Made Donation in March 2007'],df['Months since Last Donation'], marker='.', s=500)\n",
    "#plt.scatter(df['Made Donation in March 2007'],df['Number of Donations'], marker='.', s=500)\n",
    "#plt.scatter(df['Made Donation in March 2007'],df['Number of Donations'] marker='.', s=500)\n",
    "#plt.scatter(df['Made Donation in March 2007'],df['Number of Donations'] marker='.', s=500)\n",
    "#plt.xlabel('Donation')\n",
    "#plt.ylabel('Months since Last Donation')\n",
    "#plt.rcParams.update({'font.size': 40})\n",
    "#plt.xticks(np.arange(0,1.1), ('no', 'yes'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['Months since First Donation'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Made Donation in March 2007'\n",
    "out = int(math.ceil(0.009*len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre = df.drop(['Made Donation in March 2007'], axis=1)\n",
    "#X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.91073873,  7.77205216,  7.77205216,  2.6418237 ],\n",
       "       [-1.15558611,  1.32046785,  1.32046785, -0.24994586],\n",
       "       [-1.03316242,  1.84356928,  1.84356928,  0.0392311 ],\n",
       "       ...,\n",
       "       [ 1.41531137, -0.59757073, -0.59757073,  0.74151799],\n",
       "       [ 3.61893779, -0.77193788, -0.77193788,  0.20447507],\n",
       "       [ 7.65891955, -0.77193788, -0.77193788,  1.56773787]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['label']\n",
    "X = np.array(X_pre)\n",
    "X = preprocessing.scale(X) #scale data\n",
    "#df.dropna(inplace=True)\n",
    "y = np.array(df[label])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 4)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make our model!\n",
    "model = Sequential()\n",
    "model.add(Dense(100,input_shape=(4,) , activation = 'relu'))\n",
    "model.add(Dense(40,input_shape=(4,) , activation = 'relu'))\n",
    "model.add(Dense(1, activation ='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model, because this is a binary classification problem, accuracy can be used\n",
    "model.compile(optimizer='Adam', loss= 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "460/460 [==============================] - 1s 2ms/step - loss: 0.6725 - acc: 0.6109\n",
      "Epoch 2/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.6162 - acc: 0.7500\n",
      "Epoch 3/300\n",
      "460/460 [==============================] - 0s 92us/step - loss: 0.5778 - acc: 0.7500\n",
      "Epoch 4/300\n",
      "460/460 [==============================] - 0s 76us/step - loss: 0.5493 - acc: 0.7500\n",
      "Epoch 5/300\n",
      "460/460 [==============================] - 0s 62us/step - loss: 0.5317 - acc: 0.7565\n",
      "Epoch 6/300\n",
      "460/460 [==============================] - 0s 61us/step - loss: 0.5180 - acc: 0.7587\n",
      "Epoch 7/300\n",
      "460/460 [==============================] - 0s 57us/step - loss: 0.5073 - acc: 0.7630\n",
      "Epoch 8/300\n",
      "460/460 [==============================] - 0s 55us/step - loss: 0.4998 - acc: 0.7696\n",
      "Epoch 9/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4947 - acc: 0.7674\n",
      "Epoch 10/300\n",
      "460/460 [==============================] - 0s 55us/step - loss: 0.4906 - acc: 0.7696\n",
      "Epoch 11/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4879 - acc: 0.7717\n",
      "Epoch 12/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4867 - acc: 0.7804\n",
      "Epoch 13/300\n",
      "460/460 [==============================] - 0s 51us/step - loss: 0.4854 - acc: 0.7826\n",
      "Epoch 14/300\n",
      "460/460 [==============================] - 0s 63us/step - loss: 0.4850 - acc: 0.7804\n",
      "Epoch 15/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4830 - acc: 0.7804\n",
      "Epoch 16/300\n",
      "460/460 [==============================] - 0s 50us/step - loss: 0.4822 - acc: 0.7804\n",
      "Epoch 17/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4813 - acc: 0.7891\n",
      "Epoch 18/300\n",
      "460/460 [==============================] - 0s 58us/step - loss: 0.4805 - acc: 0.7891\n",
      "Epoch 19/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4800 - acc: 0.7891\n",
      "Epoch 20/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4786 - acc: 0.7957\n",
      "Epoch 21/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4778 - acc: 0.7957\n",
      "Epoch 22/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4771 - acc: 0.7957\n",
      "Epoch 23/300\n",
      "460/460 [==============================] - 0s 51us/step - loss: 0.4768 - acc: 0.7957\n",
      "Epoch 24/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4760 - acc: 0.7957\n",
      "Epoch 25/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4750 - acc: 0.7957\n",
      "Epoch 26/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4749 - acc: 0.7957\n",
      "Epoch 27/300\n",
      "460/460 [==============================] - 0s 51us/step - loss: 0.4739 - acc: 0.7957\n",
      "Epoch 28/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4734 - acc: 0.7957\n",
      "Epoch 29/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4729 - acc: 0.7978\n",
      "Epoch 30/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4726 - acc: 0.7978\n",
      "Epoch 31/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4718 - acc: 0.7978\n",
      "Epoch 32/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4716 - acc: 0.7978\n",
      "Epoch 33/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4711 - acc: 0.7978\n",
      "Epoch 34/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4704 - acc: 0.7957\n",
      "Epoch 35/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4697 - acc: 0.7957\n",
      "Epoch 36/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4694 - acc: 0.7978\n",
      "Epoch 37/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4690 - acc: 0.7978\n",
      "Epoch 38/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4687 - acc: 0.7978\n",
      "Epoch 39/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4682 - acc: 0.7957\n",
      "Epoch 40/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4678 - acc: 0.7957\n",
      "Epoch 41/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4673 - acc: 0.7957\n",
      "Epoch 42/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4683 - acc: 0.7957\n",
      "Epoch 43/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4663 - acc: 0.7957\n",
      "Epoch 44/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4662 - acc: 0.7978\n",
      "Epoch 45/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4659 - acc: 0.7957\n",
      "Epoch 46/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4655 - acc: 0.7978\n",
      "Epoch 47/300\n",
      "460/460 [==============================] - 0s 50us/step - loss: 0.4650 - acc: 0.7957\n",
      "Epoch 48/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4644 - acc: 0.7957\n",
      "Epoch 49/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4639 - acc: 0.7957\n",
      "Epoch 50/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4636 - acc: 0.7978\n",
      "Epoch 51/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4634 - acc: 0.8000\n",
      "Epoch 52/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4636 - acc: 0.8000\n",
      "Epoch 53/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4628 - acc: 0.8000\n",
      "Epoch 54/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4622 - acc: 0.8000\n",
      "Epoch 55/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4618 - acc: 0.7957\n",
      "Epoch 56/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4616 - acc: 0.7978\n",
      "Epoch 57/300\n",
      "460/460 [==============================] - 0s 56us/step - loss: 0.4610 - acc: 0.7978\n",
      "Epoch 58/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4609 - acc: 0.8000\n",
      "Epoch 59/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4612 - acc: 0.8022\n",
      "Epoch 60/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4606 - acc: 0.8000\n",
      "Epoch 61/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4603 - acc: 0.8000\n",
      "Epoch 62/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4600 - acc: 0.7978\n",
      "Epoch 63/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4599 - acc: 0.8022\n",
      "Epoch 64/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4595 - acc: 0.8000\n",
      "Epoch 65/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4589 - acc: 0.8000\n",
      "Epoch 66/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4587 - acc: 0.8000\n",
      "Epoch 67/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4585 - acc: 0.8000\n",
      "Epoch 68/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4585 - acc: 0.8000\n",
      "Epoch 69/300\n",
      "460/460 [==============================] - 0s 31us/step - loss: 0.4578 - acc: 0.8000\n",
      "Epoch 70/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4573 - acc: 0.8022\n",
      "Epoch 71/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4569 - acc: 0.7978\n",
      "Epoch 72/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4566 - acc: 0.7978\n",
      "Epoch 73/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4564 - acc: 0.8022\n",
      "Epoch 74/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4560 - acc: 0.8000\n",
      "Epoch 75/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4557 - acc: 0.8000\n",
      "Epoch 76/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4554 - acc: 0.8022\n",
      "Epoch 77/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4557 - acc: 0.7978\n",
      "Epoch 78/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4561 - acc: 0.8000\n",
      "Epoch 79/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4550 - acc: 0.8000\n",
      "Epoch 80/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4546 - acc: 0.8000\n",
      "Epoch 81/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4546 - acc: 0.8022\n",
      "Epoch 82/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4546 - acc: 0.7957\n",
      "Epoch 83/300\n",
      "460/460 [==============================] - 0s 58us/step - loss: 0.4539 - acc: 0.7978\n",
      "Epoch 84/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/460 [==============================] - 0s 35us/step - loss: 0.4538 - acc: 0.8000\n",
      "Epoch 85/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4534 - acc: 0.7957\n",
      "Epoch 86/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4533 - acc: 0.8022\n",
      "Epoch 87/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4531 - acc: 0.8022\n",
      "Epoch 88/300\n",
      "460/460 [==============================] - 0s 51us/step - loss: 0.4533 - acc: 0.8022\n",
      "Epoch 89/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4530 - acc: 0.7978\n",
      "Epoch 90/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4526 - acc: 0.8000\n",
      "Epoch 91/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4523 - acc: 0.8000\n",
      "Epoch 92/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4523 - acc: 0.7978\n",
      "Epoch 93/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4522 - acc: 0.7978\n",
      "Epoch 94/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4513 - acc: 0.8000\n",
      "Epoch 95/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4521 - acc: 0.8043\n",
      "Epoch 96/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4526 - acc: 0.8022\n",
      "Epoch 97/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4511 - acc: 0.8000\n",
      "Epoch 98/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4514 - acc: 0.7978\n",
      "Epoch 99/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4515 - acc: 0.8022\n",
      "Epoch 100/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4510 - acc: 0.8000\n",
      "Epoch 101/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4502 - acc: 0.8000\n",
      "Epoch 102/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4500 - acc: 0.8000\n",
      "Epoch 103/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4496 - acc: 0.8043\n",
      "Epoch 104/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4501 - acc: 0.8000\n",
      "Epoch 105/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4495 - acc: 0.8000\n",
      "Epoch 106/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4492 - acc: 0.8000\n",
      "Epoch 107/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4490 - acc: 0.8022\n",
      "Epoch 108/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4493 - acc: 0.8043\n",
      "Epoch 109/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4484 - acc: 0.8022\n",
      "Epoch 110/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4482 - acc: 0.7978\n",
      "Epoch 111/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4486 - acc: 0.8000\n",
      "Epoch 112/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4482 - acc: 0.8022\n",
      "Epoch 113/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4477 - acc: 0.8022\n",
      "Epoch 114/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4481 - acc: 0.8022\n",
      "Epoch 115/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4478 - acc: 0.8000\n",
      "Epoch 116/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4474 - acc: 0.7957\n",
      "Epoch 117/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4469 - acc: 0.7935\n",
      "Epoch 118/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4487 - acc: 0.8043\n",
      "Epoch 119/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4474 - acc: 0.8065\n",
      "Epoch 120/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4463 - acc: 0.7978\n",
      "Epoch 121/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4472 - acc: 0.8043\n",
      "Epoch 122/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4467 - acc: 0.7978\n",
      "Epoch 123/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4457 - acc: 0.7935\n",
      "Epoch 124/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4462 - acc: 0.7957\n",
      "Epoch 125/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4458 - acc: 0.7978\n",
      "Epoch 126/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4458 - acc: 0.8022\n",
      "Epoch 127/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4451 - acc: 0.8022\n",
      "Epoch 128/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4461 - acc: 0.8022\n",
      "Epoch 129/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4453 - acc: 0.8022\n",
      "Epoch 130/300\n",
      "460/460 [==============================] - 0s 34us/step - loss: 0.4450 - acc: 0.8065\n",
      "Epoch 131/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4451 - acc: 0.8043\n",
      "Epoch 132/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4451 - acc: 0.7978\n",
      "Epoch 133/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4441 - acc: 0.7978\n",
      "Epoch 134/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4441 - acc: 0.8022\n",
      "Epoch 135/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4439 - acc: 0.8043\n",
      "Epoch 136/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4450 - acc: 0.8065\n",
      "Epoch 137/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4440 - acc: 0.7978\n",
      "Epoch 138/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4438 - acc: 0.8043\n",
      "Epoch 139/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4437 - acc: 0.8000\n",
      "Epoch 140/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4445 - acc: 0.8087\n",
      "Epoch 141/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4443 - acc: 0.8087\n",
      "Epoch 142/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4439 - acc: 0.8043\n",
      "Epoch 143/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4434 - acc: 0.8022\n",
      "Epoch 144/300\n",
      "460/460 [==============================] - 0s 51us/step - loss: 0.4422 - acc: 0.8065\n",
      "Epoch 145/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4435 - acc: 0.8043\n",
      "Epoch 146/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4422 - acc: 0.8065\n",
      "Epoch 147/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4420 - acc: 0.8022\n",
      "Epoch 148/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4422 - acc: 0.8022\n",
      "Epoch 149/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4426 - acc: 0.8000\n",
      "Epoch 150/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4421 - acc: 0.8043\n",
      "Epoch 151/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4417 - acc: 0.8065\n",
      "Epoch 152/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4410 - acc: 0.8022\n",
      "Epoch 153/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4415 - acc: 0.8022\n",
      "Epoch 154/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4405 - acc: 0.8022\n",
      "Epoch 155/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4410 - acc: 0.8000\n",
      "Epoch 156/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4411 - acc: 0.8000\n",
      "Epoch 157/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4403 - acc: 0.8043\n",
      "Epoch 158/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4406 - acc: 0.8022\n",
      "Epoch 159/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4411 - acc: 0.8022\n",
      "Epoch 160/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4402 - acc: 0.8022\n",
      "Epoch 161/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4399 - acc: 0.8065\n",
      "Epoch 162/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4405 - acc: 0.8087\n",
      "Epoch 163/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4394 - acc: 0.8087\n",
      "Epoch 164/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4394 - acc: 0.8000\n",
      "Epoch 165/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4397 - acc: 0.8022\n",
      "Epoch 166/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4389 - acc: 0.8043\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/460 [==============================] - 0s 53us/step - loss: 0.4389 - acc: 0.8022\n",
      "Epoch 168/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4392 - acc: 0.8022\n",
      "Epoch 169/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4386 - acc: 0.8022\n",
      "Epoch 170/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4386 - acc: 0.8022\n",
      "Epoch 171/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4393 - acc: 0.8022\n",
      "Epoch 172/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4394 - acc: 0.8109\n",
      "Epoch 173/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4390 - acc: 0.8109\n",
      "Epoch 174/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4377 - acc: 0.8130\n",
      "Epoch 175/300\n",
      "460/460 [==============================] - 0s 55us/step - loss: 0.4386 - acc: 0.8022\n",
      "Epoch 176/300\n",
      "460/460 [==============================] - 0s 34us/step - loss: 0.4382 - acc: 0.8000\n",
      "Epoch 177/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4386 - acc: 0.7978\n",
      "Epoch 178/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4375 - acc: 0.8043\n",
      "Epoch 179/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4375 - acc: 0.8087\n",
      "Epoch 180/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4373 - acc: 0.8130\n",
      "Epoch 181/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4368 - acc: 0.8130\n",
      "Epoch 182/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4376 - acc: 0.8087\n",
      "Epoch 183/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4368 - acc: 0.8087\n",
      "Epoch 184/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4372 - acc: 0.8065\n",
      "Epoch 185/300\n",
      "460/460 [==============================] - 0s 51us/step - loss: 0.4364 - acc: 0.8109\n",
      "Epoch 186/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4378 - acc: 0.8109\n",
      "Epoch 187/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4362 - acc: 0.8130\n",
      "Epoch 188/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4371 - acc: 0.8043\n",
      "Epoch 189/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4357 - acc: 0.8065\n",
      "Epoch 190/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4360 - acc: 0.8130\n",
      "Epoch 191/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4362 - acc: 0.8130\n",
      "Epoch 192/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4357 - acc: 0.8065\n",
      "Epoch 193/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4367 - acc: 0.8000\n",
      "Epoch 194/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4373 - acc: 0.8065\n",
      "Epoch 195/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4364 - acc: 0.8152\n",
      "Epoch 196/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4349 - acc: 0.8130\n",
      "Epoch 197/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4360 - acc: 0.8130\n",
      "Epoch 198/300\n",
      "460/460 [==============================] - 0s 46us/step - loss: 0.4358 - acc: 0.8109\n",
      "Epoch 199/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4360 - acc: 0.8109\n",
      "Epoch 200/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4346 - acc: 0.8130\n",
      "Epoch 201/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4350 - acc: 0.8065\n",
      "Epoch 202/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4348 - acc: 0.8087\n",
      "Epoch 203/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4353 - acc: 0.8130\n",
      "Epoch 204/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4352 - acc: 0.8087\n",
      "Epoch 205/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4341 - acc: 0.8022\n",
      "Epoch 206/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4344 - acc: 0.8087\n",
      "Epoch 207/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4340 - acc: 0.8109\n",
      "Epoch 208/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4341 - acc: 0.8087\n",
      "Epoch 209/300\n",
      "460/460 [==============================] - 0s 55us/step - loss: 0.4338 - acc: 0.8109\n",
      "Epoch 210/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4342 - acc: 0.8087\n",
      "Epoch 211/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4335 - acc: 0.8087\n",
      "Epoch 212/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4341 - acc: 0.8087\n",
      "Epoch 213/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4327 - acc: 0.8109\n",
      "Epoch 214/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4345 - acc: 0.8087\n",
      "Epoch 215/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4332 - acc: 0.8065\n",
      "Epoch 216/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4338 - acc: 0.8043\n",
      "Epoch 217/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4322 - acc: 0.8109\n",
      "Epoch 218/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4324 - acc: 0.8152\n",
      "Epoch 219/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4328 - acc: 0.8087\n",
      "Epoch 220/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4320 - acc: 0.8109\n",
      "Epoch 221/300\n",
      "460/460 [==============================] - 0s 35us/step - loss: 0.4325 - acc: 0.8152\n",
      "Epoch 222/300\n",
      "460/460 [==============================] - 0s 50us/step - loss: 0.4327 - acc: 0.8196\n",
      "Epoch 223/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4316 - acc: 0.8130\n",
      "Epoch 224/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4319 - acc: 0.8087\n",
      "Epoch 225/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4320 - acc: 0.8109\n",
      "Epoch 226/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4317 - acc: 0.8087\n",
      "Epoch 227/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4319 - acc: 0.8109\n",
      "Epoch 228/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4358 - acc: 0.8022\n",
      "Epoch 229/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4297 - acc: 0.8087\n",
      "Epoch 230/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4336 - acc: 0.8152\n",
      "Epoch 231/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4330 - acc: 0.8152\n",
      "Epoch 232/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4302 - acc: 0.8087\n",
      "Epoch 233/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4334 - acc: 0.8043\n",
      "Epoch 234/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4313 - acc: 0.8065\n",
      "Epoch 235/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4327 - acc: 0.8152\n",
      "Epoch 236/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4305 - acc: 0.8130\n",
      "Epoch 237/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4314 - acc: 0.8087\n",
      "Epoch 238/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4310 - acc: 0.8087\n",
      "Epoch 239/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4308 - acc: 0.8065\n",
      "Epoch 240/300\n",
      "460/460 [==============================] - 0s 61us/step - loss: 0.4303 - acc: 0.8174\n",
      "Epoch 241/300\n",
      "460/460 [==============================] - 0s 36us/step - loss: 0.4301 - acc: 0.8109\n",
      "Epoch 242/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4293 - acc: 0.8087\n",
      "Epoch 243/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4298 - acc: 0.8065\n",
      "Epoch 244/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4294 - acc: 0.8109\n",
      "Epoch 245/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4294 - acc: 0.8130\n",
      "Epoch 246/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4293 - acc: 0.8065\n",
      "Epoch 247/300\n",
      "460/460 [==============================] - 0s 58us/step - loss: 0.4285 - acc: 0.8065\n",
      "Epoch 248/300\n",
      "460/460 [==============================] - 0s 56us/step - loss: 0.4303 - acc: 0.8130\n",
      "Epoch 249/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/460 [==============================] - 0s 62us/step - loss: 0.4304 - acc: 0.8152\n",
      "Epoch 250/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4291 - acc: 0.8130\n",
      "Epoch 251/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4289 - acc: 0.8065\n",
      "Epoch 252/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4289 - acc: 0.8043\n",
      "Epoch 253/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4293 - acc: 0.8043\n",
      "Epoch 254/300\n",
      "460/460 [==============================] - 0s 54us/step - loss: 0.4288 - acc: 0.8109\n",
      "Epoch 255/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4276 - acc: 0.8087\n",
      "Epoch 256/300\n",
      "460/460 [==============================] - 0s 50us/step - loss: 0.4283 - acc: 0.8174\n",
      "Epoch 257/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4292 - acc: 0.8152\n",
      "Epoch 258/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4281 - acc: 0.8174\n",
      "Epoch 259/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4291 - acc: 0.8087\n",
      "Epoch 260/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4276 - acc: 0.8087\n",
      "Epoch 261/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4277 - acc: 0.8196\n",
      "Epoch 262/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4288 - acc: 0.8174\n",
      "Epoch 263/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4282 - acc: 0.8087\n",
      "Epoch 264/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4282 - acc: 0.8109\n",
      "Epoch 265/300\n",
      "460/460 [==============================] - 0s 43us/step - loss: 0.4276 - acc: 0.8152\n",
      "Epoch 266/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4277 - acc: 0.8174\n",
      "Epoch 267/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4277 - acc: 0.8130\n",
      "Epoch 268/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4278 - acc: 0.8109\n",
      "Epoch 269/300\n",
      "460/460 [==============================] - 0s 42us/step - loss: 0.4264 - acc: 0.8152\n",
      "Epoch 270/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4274 - acc: 0.8174\n",
      "Epoch 271/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4272 - acc: 0.8174\n",
      "Epoch 272/300\n",
      "460/460 [==============================] - 0s 52us/step - loss: 0.4272 - acc: 0.8109\n",
      "Epoch 273/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4266 - acc: 0.8109\n",
      "Epoch 274/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4261 - acc: 0.8087\n",
      "Epoch 275/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4262 - acc: 0.8174\n",
      "Epoch 276/300\n",
      "460/460 [==============================] - 0s 53us/step - loss: 0.4263 - acc: 0.8196\n",
      "Epoch 277/300\n",
      "460/460 [==============================] - 0s 47us/step - loss: 0.4255 - acc: 0.8130\n",
      "Epoch 278/300\n",
      "460/460 [==============================] - 0s 59us/step - loss: 0.4265 - acc: 0.8109\n",
      "Epoch 279/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4259 - acc: 0.8152\n",
      "Epoch 280/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4265 - acc: 0.8109\n",
      "Epoch 281/300\n",
      "460/460 [==============================] - 0s 49us/step - loss: 0.4256 - acc: 0.8130\n",
      "Epoch 282/300\n",
      "460/460 [==============================] - 0s 45us/step - loss: 0.4265 - acc: 0.8065\n",
      "Epoch 283/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4257 - acc: 0.8152\n",
      "Epoch 284/300\n",
      "460/460 [==============================] - 0s 32us/step - loss: 0.4257 - acc: 0.8174\n",
      "Epoch 285/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4264 - acc: 0.8130\n",
      "Epoch 286/300\n",
      "460/460 [==============================] - 0s 50us/step - loss: 0.4283 - acc: 0.8109\n",
      "Epoch 287/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4253 - acc: 0.8152\n",
      "Epoch 288/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4250 - acc: 0.8087\n",
      "Epoch 289/300\n",
      "460/460 [==============================] - 0s 40us/step - loss: 0.4273 - acc: 0.8087\n",
      "Epoch 290/300\n",
      "460/460 [==============================] - 0s 39us/step - loss: 0.4245 - acc: 0.8022\n",
      "Epoch 291/300\n",
      "460/460 [==============================] - 0s 38us/step - loss: 0.4241 - acc: 0.8087\n",
      "Epoch 292/300\n",
      "460/460 [==============================] - 0s 33us/step - loss: 0.4248 - acc: 0.8152\n",
      "Epoch 293/300\n",
      "460/460 [==============================] - 0s 71us/step - loss: 0.4243 - acc: 0.8174\n",
      "Epoch 294/300\n",
      "460/460 [==============================] - 0s 62us/step - loss: 0.4248 - acc: 0.8065\n",
      "Epoch 295/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4244 - acc: 0.8043\n",
      "Epoch 296/300\n",
      "460/460 [==============================] - 0s 44us/step - loss: 0.4235 - acc: 0.8065\n",
      "Epoch 297/300\n",
      "460/460 [==============================] - 0s 56us/step - loss: 0.4244 - acc: 0.8130\n",
      "Epoch 298/300\n",
      "460/460 [==============================] - 0s 41us/step - loss: 0.4240 - acc: 0.8109\n",
      "Epoch 299/300\n",
      "460/460 [==============================] - 0s 37us/step - loss: 0.4244 - acc: 0.8174\n",
      "Epoch 300/300\n",
      "460/460 [==============================] - 0s 48us/step - loss: 0.4243 - acc: 0.8087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x102a1ac18>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model\n",
    "model.fit(X_train, y_train, batch_size = 100, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 1ms/step\n",
      "[0.47435109676985904, 0.8362069006623893]\n"
     ]
    }
   ],
   "source": [
    "#Let's Evaluate our model\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(score)\n",
    "#len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "pred = model.predict([X_test])\n",
    "pred = pred.round(0)\n",
    "prediction = pd.DataFrame(pred, columns = ['pred']) \n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's put the real Adj Close values of our predictions into a dataframe\n",
    "compare = pd.DataFrame(y_test,columns = ['real'])\n",
    "compare = compare.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred = compare.join(prediction)\n",
    "#plot_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.concat([prediction, compare], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = new['real']\n",
    "ii= new['pred']\n",
    "Correct_guessed = new.where(i == ii)\n",
    "Correct_guessed.dropna(inplace =True)\n",
    "#Correct_guessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/anyaozmen/Downloads/blood_don/test.csv'\n",
    "n_file = pd.read_csv(file, index_col = 0)\n",
    "#n_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.98618442,  0.99265264,  0.99265264,  0.65972434],\n",
       "       [ 1.51381612,  0.17430751,  0.17430751,  0.10080508],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 0.19802636,  0.82898361,  0.82898361,  0.10080508],\n",
       "       [-0.72302647,  0.99265264,  0.99265264, -0.05888614],\n",
       "       [-0.85460545,  2.46567387,  2.46567387,  0.2604963 ],\n",
       "       [-0.72302647, -0.64403762, -0.64403762, -1.25657026],\n",
       "       [ 0.59276329, -0.80770664, -0.80770664, -0.85734222],\n",
       "       [ 1.77697407, -0.64403762, -0.64403762,  2.05702248],\n",
       "       [ 0.59276329, -0.31669956, -0.31669956,  1.13879799],\n",
       "       [ 0.46118431, -0.48036859, -0.48036859, -0.77749661],\n",
       "       [ 0.19802636,  0.17430751,  0.17430751,  1.05895238],\n",
       "       [-0.5914475 ,  0.82898361,  0.82898361,  1.57794883],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.72302647, -0.31669956, -0.31669956, -0.37826857],\n",
       "       [ 0.19802636, -0.80770664, -0.80770664, -0.97711063],\n",
       "       [ 0.19802636,  0.01063849,  0.01063849, -0.37826857],\n",
       "       [-0.32828954,  1.31999069,  1.31999069,  0.50003312],\n",
       "       [ 1.77697407,  1.31999069,  1.31999069,  2.2965593 ],\n",
       "       [-0.85460545, -0.31669956, -0.31669956, -0.25850016],\n",
       "       [-0.98618442,  0.17430751,  0.17430751, -0.25850016],\n",
       "       [-0.72302647,  0.01063849,  0.01063849, -0.01896333],\n",
       "       [-0.5914475 ,  0.17430751,  0.17430751, -0.37826857],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.98618442, -0.48036859, -0.48036859,  0.10080508],\n",
       "       [-0.5914475 ,  1.31999069,  1.31999069,  2.01709967],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -0.97711063],\n",
       "       [ 0.59276329, -0.80770664, -0.80770664, -0.85734222],\n",
       "       [-0.72302647, -0.48036859, -0.48036859, -0.77749661],\n",
       "       [-0.98618442,  0.99265264,  0.99265264,  0.65972434],\n",
       "       [-0.72302647,  1.31999069,  1.31999069,  2.01709967],\n",
       "       [ 1.77697407,  0.17430751,  0.17430751,  2.09694528],\n",
       "       [-0.98618442, -0.80770664, -0.80770664, -1.33641587],\n",
       "       [-0.72302647,  0.17430751,  0.17430751,  0.89926116],\n",
       "       [-0.72302647, -0.64403762, -0.64403762,  0.22057349],\n",
       "       [ 0.19802636,  0.17430751,  0.17430751, -0.25850016],\n",
       "       [-0.98618442, -0.64403762, -0.64403762,  0.22057349],\n",
       "       [ 0.59276329, -0.80770664, -0.80770664, -0.85734222],\n",
       "       [-0.72302647,  0.01063849,  0.01063849, -0.29842296],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -1.25657026],\n",
       "       [ 0.85592124,  0.01063849,  0.01063849, -0.01896333],\n",
       "       [ 0.85592124,  0.01063849,  0.01063849,  1.81748565],\n",
       "       [-0.98618442, -0.15303054, -0.15303054, -0.37826857],\n",
       "       [ 0.59276329, -0.48036859, -0.48036859, -0.17865455],\n",
       "       [-0.98618442, -0.80770664, -0.80770664, -1.33641587],\n",
       "       [-0.72302647, -0.31669956, -0.31669956, -0.85734222],\n",
       "       [ 0.59276329, -0.48036859, -0.48036859, -0.01896333],\n",
       "       [ 0.59276329, -0.80770664, -0.80770664, -0.85734222],\n",
       "       [ 0.59276329, -0.15303054, -0.15303054, -0.29842296],\n",
       "       [-0.98618442,  1.31999069,  1.31999069,  0.85933836],\n",
       "       [-0.5914475 ,  2.95668095,  2.95668095,  1.73764005],\n",
       "       [ 0.59276329, -0.31669956, -0.31669956, -0.49803698],\n",
       "       [-0.72302647,  0.01063849,  0.01063849,  0.14072788],\n",
       "       [ 1.77697407, -0.64403762, -0.64403762,  0.10080508],\n",
       "       [ 0.19802636,  0.33797654,  0.33797654,  0.65972434],\n",
       "       [-0.98618442,  0.17430751,  0.17430751,  1.65779444],\n",
       "       [-0.72302647, -0.15303054, -0.15303054, -0.97711063],\n",
       "       [-0.98618442, -0.31669956, -0.31669956, -0.01896333],\n",
       "       [-0.98618442,  1.48365972,  1.48365972,  1.13879799],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 0.19802636, -0.64403762, -0.64403762,  0.10080508],\n",
       "       [ 0.32960533,  1.48365972,  1.48365972,  1.41825761],\n",
       "       [-0.98618442,  1.15632167,  1.15632167,  1.61787163],\n",
       "       [ 0.19802636, -0.64403762, -0.64403762,  0.10080508],\n",
       "       [ 0.85592124, -0.64403762, -0.64403762, -0.33834576],\n",
       "       [-0.06513159, -0.31669956, -0.31669956,  1.17872079],\n",
       "       [ 1.51381612,  1.64732874,  1.64732874,  1.13879799],\n",
       "       [-0.32828954,  0.66531459,  0.66531459,  0.46011032],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 0.19802636, -0.15303054, -0.15303054, -0.01896333],\n",
       "       [ 0.85592124, -0.31669956, -0.31669956, -0.49803698],\n",
       "       [ 3.75065871, -0.80770664, -0.80770664,  0.10080508],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 1.77697407, -0.31669956, -0.31669956,  0.65972434],\n",
       "       [ 0.19802636,  0.17430751,  0.17430751,  1.13879799],\n",
       "       [ 0.59276329, -0.48036859, -0.48036859, -0.29842296],\n",
       "       [-0.72302647,  0.82898361,  0.82898361,  1.69771724],\n",
       "       [ 0.85592124, -0.31669956, -0.31669956, -0.09880894],\n",
       "       [-0.72302647, -0.15303054, -0.15303054, -0.97711063],\n",
       "       [ 3.75065871, -0.80770664, -0.80770664,  0.10080508],\n",
       "       [ 0.19802636,  0.82898361,  0.82898361,  0.10080508],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -0.97711063],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -1.25657026],\n",
       "       [-0.98618442,  1.31999069,  1.31999069,  0.85933836],\n",
       "       [ 1.51381612, -0.80770664, -0.80770664, -0.57788259],\n",
       "       [ 0.19802636, -0.80770664, -0.80770664, -0.97711063],\n",
       "       [ 0.85592124, -0.64403762, -0.64403762, -0.37826857],\n",
       "       [ 1.51381612,  1.64732874,  1.64732874,  1.13879799],\n",
       "       [-0.98618442, -0.31669956, -0.31669956, -0.37826857],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.72302647, -0.64403762, -0.64403762,  0.65972434],\n",
       "       [ 0.59276329, -0.48036859, -0.48036859, -0.17865455],\n",
       "       [ 0.85592124, -0.64403762, -0.64403762, -0.77749661],\n",
       "       [ 0.85592124,  0.82898361,  0.82898361,  0.18065069],\n",
       "       [ 0.19802636,  0.82898361,  0.82898361,  0.2604963 ],\n",
       "       [-0.72302647, -0.15303054, -0.15303054, -0.49803698],\n",
       "       [ 0.19802636,  0.99265264,  0.99265264,  0.89926116],\n",
       "       [ 1.77697407,  0.33797654,  0.33797654,  1.13879799],\n",
       "       [-0.72302647, -0.64403762, -0.64403762, -1.25657026],\n",
       "       [-0.98618442, -0.80770664, -0.80770664, -1.33641587],\n",
       "       [ 0.19802636,  0.01063849,  0.01063849,  0.89926116],\n",
       "       [ 1.51381612, -0.48036859, -0.48036859, -0.01896333],\n",
       "       [-0.32828954, -0.15303054, -0.15303054, -0.01896333],\n",
       "       [ 0.19802636, -0.64403762, -0.64403762, -0.77749661],\n",
       "       [-0.98618442, -0.31669956, -0.31669956, -0.97711063],\n",
       "       [ 0.72434226,  1.64732874,  1.64732874,  1.85740846],\n",
       "       [-0.98618442,  0.66531459,  0.66531459,  0.53995592],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 0.85592124, -0.48036859, -0.48036859, -0.57788259],\n",
       "       [-0.06513159, -0.64403762, -0.64403762, -0.77749661],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 1.77697407,  0.17430751,  0.17430751,  2.09694528],\n",
       "       [-0.72302647,  0.33797654,  0.33797654, -0.29842296],\n",
       "       [ 0.19802636,  0.82898361,  0.82898361,  0.10080508],\n",
       "       [ 1.77697407, -0.48036859, -0.48036859,  0.50003312],\n",
       "       [ 1.77697407, -0.80770664, -0.80770664, -0.49803698],\n",
       "       [-0.72302647,  0.17430751,  0.17430751, -0.29842296],\n",
       "       [-0.72302647,  1.64732874,  1.64732874,  0.10080508],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -0.49803698],\n",
       "       [ 0.19802636,  0.01063849,  0.01063849,  0.89926116],\n",
       "       [-0.72302647,  0.50164556,  0.50164556, -0.37826857],\n",
       "       [ 0.19802636, -0.31669956, -0.31669956, -0.05888614],\n",
       "       [-1.24934238,  3.284019  ,  3.284019  ,  1.61787163],\n",
       "       [-0.98618442,  0.66531459,  0.66531459,  0.65972434],\n",
       "       [-0.98618442,  1.64732874,  1.64732874,  1.81748565],\n",
       "       [-0.72302647,  0.01063849,  0.01063849,  0.42018751],\n",
       "       [ 0.19802636, -0.64403762, -0.64403762, -0.97711063],\n",
       "       [ 0.19802636, -0.31669956, -0.31669956, -0.05888614],\n",
       "       [ 0.59276329, -0.64403762, -0.64403762, -0.85734222],\n",
       "       [-0.85460545, -0.15303054, -0.15303054, -0.37826857],\n",
       "       [ 0.19802636, -0.80770664, -0.80770664, -0.97711063],\n",
       "       [ 0.19802636, -0.48036859, -0.48036859, -0.81741941],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.72302647,  0.17430751,  0.17430751,  0.89926116],\n",
       "       [-0.72302647, -0.64403762, -0.64403762, -1.25657026],\n",
       "       [ 0.19802636, -0.80770664, -0.80770664, -0.97711063],\n",
       "       [ 0.19802636, -0.48036859, -0.48036859,  1.61787163],\n",
       "       [-0.98618442,  1.15632167,  1.15632167, -0.13873174],\n",
       "       [-0.98618442,  0.99265264,  0.99265264,  2.49617332],\n",
       "       [-0.98618442,  4.5933712 ,  4.5933712 ,  1.65779444],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -0.97711063],\n",
       "       [-0.98618442,  0.17430751,  0.17430751,  1.65779444],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 1.77697407, -0.80770664, -0.80770664, -0.49803698],\n",
       "       [ 1.77697407, -0.48036859, -0.48036859, -0.01896333],\n",
       "       [-0.72302647, -0.15303054, -0.15303054, -0.09880894],\n",
       "       [ 1.77697407, -0.48036859, -0.48036859,  1.05895238],\n",
       "       [-0.98618442, -0.80770664, -0.80770664, -1.33641587],\n",
       "       [ 1.51381612, -0.64403762, -0.64403762,  0.22057349],\n",
       "       [-0.98618442,  0.50164556,  0.50164556, -0.53795978],\n",
       "       [ 0.19802636,  0.50164556,  0.50164556, -0.09880894],\n",
       "       [ 0.85592124,  0.01063849,  0.01063849,  0.18065069],\n",
       "       [ 0.85592124, -0.48036859, -0.48036859, -0.6577282 ],\n",
       "       [-0.19671057,  1.48365972,  1.48365972,  1.65779444],\n",
       "       [ 0.85592124, -0.80770664, -0.80770664, -0.77749661],\n",
       "       [-0.98618442, -0.80770664, -0.80770664, -1.33641587],\n",
       "       [-0.5914475 ,  2.95668095,  2.95668095,  1.73764005],\n",
       "       [ 0.19802636, -0.80770664, -0.80770664, -0.97711063],\n",
       "       [ 0.32960533,  1.48365972,  1.48365972,  1.41825761],\n",
       "       [ 0.59276329, -0.80770664, -0.80770664, -0.85734222],\n",
       "       [-0.72302647,  0.01063849,  0.01063849, -0.49803698],\n",
       "       [-0.06513159,  0.50164556,  0.50164556, -0.77749661],\n",
       "       [-0.98618442, -0.48036859, -0.48036859,  0.65972434],\n",
       "       [ 0.85592124, -0.48036859, -0.48036859,  0.57987873],\n",
       "       [-0.98618442, -0.64403762, -0.64403762, -0.97711063],\n",
       "       [-0.72302647,  1.31999069,  1.31999069,  2.01709967],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.85460545, -0.31669956, -0.31669956, -0.25850016],\n",
       "       [ 1.77697407, -0.80770664, -0.80770664, -0.49803698],\n",
       "       [ 4.01381666, -0.80770664, -0.80770664,  0.18065069],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [-0.98618442,  5.73905438,  5.73905438,  2.49617332],\n",
       "       [-0.72302647, -0.64403762, -0.64403762,  0.65972434],\n",
       "       [ 1.51381612, -0.48036859, -0.48036859,  0.10080508],\n",
       "       [-0.98618442, -0.48036859, -0.48036859, -1.05695624],\n",
       "       [-0.98618442,  0.99265264,  0.99265264,  2.37640491],\n",
       "       [ 0.19802636, -0.64403762, -0.64403762,  0.65972434],\n",
       "       [-0.72302647, -0.80770664, -0.80770664, -1.25657026],\n",
       "       [ 0.59276329, -0.80770664, -0.80770664, -0.85734222],\n",
       "       [ 0.19802636, -0.15303054, -0.15303054, -0.09880894],\n",
       "       [-0.72302647,  0.50164556,  0.50164556, -0.37826857],\n",
       "       [ 0.85592124, -0.80770664, -0.80770664, -0.77749661],\n",
       "       [-0.72302647,  1.64732874,  1.64732874,  1.37833481],\n",
       "       [ 0.59276329, -0.64403762, -0.64403762, -0.85734222],\n",
       "       [ 0.59276329, -0.48036859, -0.48036859, -0.29842296],\n",
       "       [ 0.19802636, -0.80770664, -0.80770664, -0.97711063],\n",
       "       [-0.98618442, -0.80770664, -0.80770664, -1.33641587],\n",
       "       [ 0.59276329, -0.64403762, -0.64403762, -0.85734222],\n",
       "       [-0.72302647,  2.79301192,  2.79301192,  0.89926116],\n",
       "       [-0.45986852, -0.48036859, -0.48036859, -0.37826857],\n",
       "       [ 0.85592124, -0.64403762, -0.64403762, -0.77749661],\n",
       "       [ 0.59276329, -0.64403762, -0.64403762, -0.25850016],\n",
       "       [ 0.19802636,  0.17430751,  0.17430751,  1.13879799],\n",
       "       [ 0.85592124, -0.80770664, -0.80770664, -0.77749661],\n",
       "       [ 1.51381612, -0.64403762, -0.64403762, -0.49803698],\n",
       "       [ 1.77697407,  0.33797654,  0.33797654,  0.42018751],\n",
       "       [ 1.77697407, -0.64403762, -0.64403762, -0.29842296],\n",
       "       [-0.72302647,  0.82898361,  0.82898361,  1.13879799]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown = np.array(n_file)\n",
    "unknown = preprocessing.scale(unknown)\n",
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = model.predict([unknown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_predictions.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = pd.DataFrame(final_predictions, columns=['Made Donation in March 2007'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file.to_csv('/Users/anyaozmen/Desktop/final_donation_predictions_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
